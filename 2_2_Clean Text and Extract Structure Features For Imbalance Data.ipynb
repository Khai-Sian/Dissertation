{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ed8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import mwparserfromhell\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2efa8",
   "metadata": {},
   "source": [
    "# Wikiextractor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99982c",
   "metadata": {},
   "source": [
    "## Fix WikiExtractor file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79578037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_root_if_missing(file_path):\n",
    "    \"\"\"\n",
    "    The file extracted by WikiExtractor have no root element\n",
    "    Add root element in the file so it can work as xml\n",
    "    \n",
    "    file_path: the path of WikiExtractor file\n",
    "    \"\"\"\n",
    "\n",
    "    # read file line by line\n",
    "    with open(file_path, 'r', encoding='latin1') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # check if the first line contains the root element <documents>\n",
    "    if not any('<documents>' in line for line in lines):\n",
    "        # wrap the content with a root element\n",
    "        corrected_content = f\"<documents>\\n{''.join(lines)}\\n</documents>\"\n",
    "    else:\n",
    "        # if the root element is already added, use the original content\n",
    "        corrected_content = ''.join(lines)\n",
    "        print(\"Root tag already exists\")\n",
    "\n",
    "    # write the corrected content back to the file\n",
    "    with open(file_path, 'w', encoding='latin1') as file:\n",
    "        file.write(corrected_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3ee17",
   "metadata": {},
   "source": [
    "## Get clean text from WikiExtractor file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce9cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_wikiextractor(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    articles = {}\n",
    "    for doc in root.findall('doc'): # find each article\n",
    "        title = doc.get('title')\n",
    "        text = doc.text.strip()\n",
    "        articles[title] = text # create dictionary item based on title\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdb091",
   "metadata": {},
   "source": [
    "# Extract Feature Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d16e7a",
   "metadata": {},
   "source": [
    "## Get number of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a182c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_reference(wiki):\n",
    "    \"\"\"\n",
    "    Get number of reference in each articles\n",
    "    \n",
    "    wiki: wiki object\n",
    "    \"\"\"\n",
    "   \n",
    "    # find ref tag\n",
    "    references = wiki.filter_tags(matches=lambda node: node.tag == \"ref\")\n",
    "    \n",
    "    ref_name = []\n",
    "    unique_ref = []\n",
    "    \n",
    "    for ref in references:\n",
    "        if len([att for att in ref.attributes if 'name=\\\"' in att]): # if reference have name\n",
    "            attribute = [att for att in ref.attributes if 'name=\\\"' in att][0] # get the name of reference\n",
    "            \n",
    "            if attribute not in ref_name: # if reference name is new\n",
    "                unique_ref.append(ref)\n",
    "                ref_name.append(attribute)\n",
    "        else:\n",
    "            unique_ref.append(ref)\n",
    "            \n",
    "    return len(unique_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b6dad",
   "metadata": {},
   "source": [
    "## Get number of external and internal links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d651684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_links(wiki):\n",
    "    ex_links = wiki.filter_external_links()\n",
    "    wiki_links = wiki.filter_wikilinks()\n",
    "    \n",
    "    return len(ex_links), len(wiki_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993548b",
   "metadata": {},
   "source": [
    "## Get number of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee478e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_tables(text):\n",
    "    # find table in this format {| |}\n",
    "    pattern = re.compile(r'\\{\\|.*?\\|\\}', re.DOTALL) # DOTALL make . also matches \\n\n",
    "    tables = pattern.findall(text)\n",
    "    return len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e664e07",
   "metadata": {},
   "source": [
    "## Get number of formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7968c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_formula(wiki):\n",
    "    formulas = wiki.filter_tags(matches=lambda node: node.tag == \"math\")\n",
    "    return len(formulas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be090441",
   "metadata": {},
   "source": [
    "## Get number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29d0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_image(text):\n",
    "    # find image in this format [[File: ]]\n",
    "    pattern = re.compile(r'\\[\\[File:(.*?)\\]\\]')\n",
    "    images = pattern.findall(text)\n",
    "    return len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de004a6",
   "metadata": {},
   "source": [
    "## Get number of sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56a29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_section(text, prefix):\n",
    "    # find section in this format<!--== ==> / <!--=== ===> \n",
    "    pattern = re.compile(f'(?<!--){prefix}([^=]+){prefix}(?!=)')\n",
    "    sections = pattern.findall(text)\n",
    "    sections = [re.sub(r'[^A-Za-z ]', '', section).strip() for section in sections]\n",
    "    return len(sections), sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36c198",
   "metadata": {},
   "source": [
    "## Get number of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5dafaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_paragraph(text, total_section):\n",
    "    paragraph = text.split('\\n')\n",
    "    # find paragraph that is not in this format formula_\n",
    "    pattern = re.compile(r'formula_\\d+')\n",
    "    filtered_paragraph = [p for p in paragraph if p != \"\" and not pattern.match(p) and p.strip()[:-1] not in total_section]\n",
    "    return len(filtered_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c87133",
   "metadata": {},
   "source": [
    "## Get number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d7ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_sentence(text, total_section):\n",
    "    # same as find paragraph\n",
    "    paragraph = text.split('\\n')\n",
    "    # find paragraph that is not in this format formula_\n",
    "    pattern = re.compile(r'formula_\\d+')\n",
    "    filtered_paragraph = [p for p in paragraph if p != \"\" and not pattern.match(p) and p.strip()[:-1] not in total_section]\n",
    "    \n",
    "    # separate paragraph by sentence\n",
    "    filtered_text = ' '.join(filtered_paragraph)\n",
    "    sentences = sent_tokenize(filtered_text)\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e490e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Clean Text and add Structure Features\n",
    "    \n",
    "    df: dataframe of articles\n",
    "    \"\"\"\n",
    "    \n",
    "    add_root_if_missing('D:/Leeds/Dissertation/Data/Wiki Dumps/articles/AA/wiki_00')\n",
    "    \n",
    "    print(\"Start Getting Clean Text from extracted Wikiextractor file ....\")\n",
    "    start_time = time.time()\n",
    "    articles = read_file_from_wikiextractor('D:/Leeds/Dissertation/Data/Wiki Dumps/articles/AA/wiki_00')\n",
    "    df['clean_text'] = df['title'].map(articles)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Get Clean Text time: {elapsed_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Remove NaN value from clean text\n",
    "    print(\"NaN clean text: \", len(np.where(df['clean_text'].isnull())[0]))\n",
    "    df = df[df['clean_text'].notna()]\n",
    "    print(\"Number of rows: \", df.shape[0])\n",
    "    \n",
    "    print(\"Start Getting Structure Features ....\")\n",
    "    start_time = time.time()\n",
    "    for index, row in df.iterrows():\n",
    "        wiki = mwparserfromhell.parse(row['text']) # convert to wiki object\n",
    "        text = row['text']\n",
    "        clean_text = row['clean_text']\n",
    "\n",
    "        df.loc[index, 'reference'] = number_of_reference(wiki)\n",
    "        \n",
    "        ex_links, wiki_links = number_of_links(wiki)\n",
    "        df.loc[index, 'external_link'] = ex_links\n",
    "        df.loc[index, 'internal_link'] = wiki_links\n",
    "        \n",
    "        df.loc[index, 'table'] = number_of_tables(text)\n",
    "        df.loc[index, 'formula'] = number_of_formula(wiki)\n",
    "        df.loc[index, 'images'] = number_of_image(text)\n",
    "    \n",
    "        # Count number of sections, subsections, sub-subsections based on number of = \n",
    "        section_numb, section = number_of_section(text, '==')\n",
    "        subsection_numb, subsection = number_of_section(text, '===')\n",
    "        subsubsection_numb, subsubsection = number_of_section(text, '====')\n",
    "        \n",
    "        df.loc[index, 'section'] = section_numb\n",
    "        df.loc[index, 'subsection'] = subsection_numb\n",
    "        df.loc[index, 'subsubsection'] = subsubsection_numb\n",
    "\n",
    "        total_section = section + subsection + subsubsection\n",
    "        df.loc[index, 'paragraph'] = number_of_paragraph(clean_text, total_section)\n",
    "        df.loc[index, 'sentence'] = number_of_sentence(clean_text, total_section)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Get Structure Features time: {elapsed_time/60:.2f} minutes\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef59c998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file alreday exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../Data/dataset_text_structure_(Imbalance).csv'):\n",
    "    df = pd.read_csv('../Data/initial_dataset_(Imbalance).csv', keep_default_na=False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df = add_features(df)\n",
    "    df.to_csv('../Data/dataset_text_structure_(Imbalance).csv', index=False)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"CSV file created\")\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time/60:.2f} minutes\")\n",
    "else:\n",
    "    df = pd.read_csv(f'../Data/dataset_text_structure_(Imbalance).csv', keep_default_na=False)\n",
    "    print(\"CSV file alreday exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b06e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19181, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
