{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fe4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "\n",
    "from lxml import etree \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f89745",
   "metadata": {},
   "source": [
    "# Show Wikimedia dumps on different dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab590346",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://dumps.wikimedia.org/enwiki/\"\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508d7624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '20240320/',\n",
       " '20240401/',\n",
       " '20240420/',\n",
       " '20240501/',\n",
       " '20240601/',\n",
       " '20240620/',\n",
       " '20240701/',\n",
       " 'latest/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = [a['href'] for a in soup_index.find_all('a') if a.has_attr('href')]\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25bb61",
   "metadata": {},
   "source": [
    "# Shows various dumps on that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01cbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = base_url + \"20240620/\"\n",
    "dump_html = requests.get(dump_url).text\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629a5020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20240620-pages-articles-multistream.xml.bz2', ['22.1', 'GB']),\n",
       " ('enwiki-20240620-pages-articles-multistream-index.txt.bz2', ['249.0', 'MB']),\n",
       " ('enwiki-20240620-pages-articles-multistream1.xml-p1p41242.bz2',\n",
       "  ['269.0', 'MB']),\n",
       " ('enwiki-20240620-pages-articles-multistream-index1.txt-p1p41242.bz2',\n",
       "  ['221', 'KB']),\n",
       " ('enwiki-20240620-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
       "  ['358.6', 'MB'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a315d856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-20240620-pages-articles-multistream1.xml-p1p41242.bz2',\n",
       " 'enwiki-20240620-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
       " 'enwiki-20240620-pages-articles-multistream3.xml-p151574p311329.bz2',\n",
       " 'enwiki-20240620-pages-articles-multistream4.xml-p311330p558391.bz2',\n",
       " 'enwiki-20240620-pages-articles-multistream5.xml-p558392p958045.bz2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_files = [file[0] for file in files if ('.xml-p' in file[0]) and ('rss' not in file[0])]\n",
    "dump_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea8052",
   "metadata": {},
   "source": [
    "# Download dump if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945f3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwiki-20240620-pages-articles-multistream1.xml-p1p41242.bz2 is already downloaded and complete.\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'D:/Leeds/Dissertation/Data/Wiki Dumps/'\n",
    "\n",
    "file = dump_files[0] # Only download the first file\n",
    "file_path = folder_path + file\n",
    "\n",
    "response = requests.head(dump_url + file)\n",
    "file_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    if os.path.getsize(file_path) == file_size:\n",
    "        print(f\"{file} is already downloaded and complete.\")\n",
    "    else:\n",
    "        print(f\"{file}' is incomplete. Re-downloading...\")\n",
    "        os.remove(file_path)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print('Downloading...')\n",
    "    with requests.get(dump_url + file, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                \n",
    "    if os.path.getsize(file_path) == file_size:\n",
    "        print(f\"{file} downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Error: File '{file}' downloaded but is incomplete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226fd5b",
   "metadata": {},
   "source": [
    "# Extract the downloaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724bf0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exists\n"
     ]
    }
   ],
   "source": [
    "output_path = folder_path + file[:-4] # remove the file extension\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    print('Extracting')\n",
    "    with bz2.open(file_path, 'rb') as f_in:\n",
    "        with open(output_path, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "else:\n",
    "    print('Files already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6a7ba",
   "metadata": {},
   "source": [
    "# Extract articles title and text from xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f7409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = '{http://www.mediawiki.org/xml/export-0.11/}'\n",
    "\n",
    "page_tag = f'{namespace}page'\n",
    "title_tag = f'{namespace}title'\n",
    "id_tag = f'{namespace}revision/{namespace}id'\n",
    "text_tag = f'{namespace}revision/{namespace}text'\n",
    "\n",
    "def parse_wikipedia_dump(dump_file):\n",
    "    \"\"\"\n",
    "    Extract articles in dump file and convert to list of dictionary\n",
    "    \n",
    "    dump_file: file to be extracted\n",
    "    \"\"\"\n",
    "    context = etree.iterparse(dump_file, events=('end',), tag=page_tag)\n",
    "    articles = []\n",
    "    \n",
    "    for event, elem in context:\n",
    "        if (elem.find(title_tag) is not None) & (elem.find(id_tag) is not None) & (elem.find(text_tag) is not None):\n",
    "            title = elem.find(title_tag).text\n",
    "            pageid = elem.find(id_tag).text\n",
    "            text = elem.find(text_tag).text\n",
    "        \n",
    "            articles.append({'title': title, 'pageid': pageid, 'text':text})\n",
    "        elem.clear()\n",
    "        \n",
    "    del context\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef28885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27374  articles found\n"
     ]
    }
   ],
   "source": [
    "articles = parse_wikipedia_dump(output_path)\n",
    "print(len(articles), \" articles found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20d1e2",
   "metadata": {},
   "source": [
    "# Get Wikiproject assessment based on article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e318abf",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def handle_batch_articles(batch_article_list, articles):\n",
    "    \"\"\"\n",
    "    Handle a single batch of articles\n",
    "    \n",
    "    batch_article_list: json return from the API\n",
    "    articles: list of articles extracted above\n",
    "    \"\"\"\n",
    "    article_no_rating_list = []\n",
    "    complete_article_list = []\n",
    "    \n",
    "    for page_info in batch_article_list.values(): # page_info represent an article\n",
    "        title = page_info['title'] if 'title' in page_info else \"\"\n",
    "\n",
    "        # get the article text based on title\n",
    "        for article in articles:\n",
    "            if article['title'] == title:\n",
    "                text = article['text']\n",
    "                break\n",
    "\n",
    "        # list of assessments/ratings\n",
    "        assessment_list = list(page_info['pageassessments'].values()) if 'pageassessments' in page_info else []\n",
    "        \n",
    "        rate = \"\"\n",
    "        importance = \"\"\n",
    "        if len(assessment_list) > 0:\n",
    "            for assessment in assessment_list:\n",
    "                if assessment['class'] and assessment['importance']: # if rate and importance is not empty\n",
    "                    rate = assessment['class']\n",
    "                    importance = assessment['importance']\n",
    "                    break\n",
    "                elif assessment['class']: # if only rate is not empty\n",
    "                    rate = assessment['class']\n",
    "                    importance = \"Unknown\" # classify articles importance as 'Unknown'\n",
    "        else:\n",
    "            article_no_rating_list.append(title)\n",
    "        \n",
    "        if title and text and rate and importance: # if all information is complete\n",
    "            article_data = {'title': title,\n",
    "                            'text': text,\n",
    "                            'rate': rate,\n",
    "                            'importance': importance}\n",
    "            complete_article_list.append(article_data)\n",
    "            \n",
    "    return complete_article_list, article_no_rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07948e89",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fetch_batch_article_details(articles):\n",
    "    \"\"\"\n",
    "    Separate list of articles to batch of 4 \n",
    "    (4 is maximum where API can return complete information)\n",
    "    \n",
    "    articles: list of articles extracted above\n",
    "    \"\"\"\n",
    "    \n",
    "    def batches(titles, n = 4):\n",
    "        \"\"\"\n",
    "        Separate list of titles to batch of 4\n",
    "        \n",
    "        titles: article titles is split into batches\n",
    "        \"\"\"\n",
    "        for i in range(0, len(titles), n):\n",
    "            yield titles[i:i+n]\n",
    "    \n",
    "    # variables to show processing progress\n",
    "    threshold_step = 1000\n",
    "    next_threshold = threshold_step\n",
    "    \n",
    "    \n",
    "    batch_article_list = [] # list of articles with complete information\n",
    "    batch_article_no_rating_list = [] # list of articles without rating\n",
    "    \n",
    "    titles = [article['title'] for article in articles]\n",
    "    \n",
    "    for batch_titles in batches(titles):\n",
    "        titles_query = \"|\".join(map(str, batch_titles))\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\"action\": \"query\",\n",
    "                  \"format\": \"json\",\n",
    "                  \"prop\": \"pageassessments\",\n",
    "                  \"rvprop\": \"content\",\n",
    "                  \"titles\": titles_query}\n",
    "        \n",
    "        response = requests.get(url, params=params).json()\n",
    "            \n",
    "        complete_article_list, article_no_rating_list = handle_batch_articles(response['query']['pages'], articles)\n",
    "        batch_article_list.extend(complete_article_list)\n",
    "        batch_article_no_rating_list.extend(article_no_rating_list)\n",
    "        \n",
    "        # Show progress\n",
    "        if len(batch_article_list) >= next_threshold:\n",
    "            next_threshold += threshold_step\n",
    "            print(f\"We are have processed {len(batch_article_list)} articles\")\n",
    "    print(f\"Abandoned {len(batch_article_no_rating_list)} articles\")\n",
    "        \n",
    "    return batch_article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6636f1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    \"\"\"\n",
    "    Create initial dataset\n",
    "    \"\"\"\n",
    "    articles_with_ratings = []\n",
    "\n",
    "    articles_with_ratings.extend(fetch_batch_article_details(articles))\n",
    "\n",
    "    df = pd.DataFrame(articles_with_ratings)\n",
    "    df.to_csv('../Data/initial_dataset_(Imbalance).csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c95f9d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file alreday exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../Data/initial_dataset_(Imbalance).csv'):\n",
    "    print(\"Start Scrapping Data from Wikimedia dumps ....\")\n",
    "    start_time = time.time()\n",
    "    df = create_data()\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Scrape Data time: {elapsed_time/60:.2f} minutes\")\n",
    "    print(\"CSV file created\")\n",
    "else:\n",
    "    df = pd.read_csv('../Data/initial_dataset_(Imbalance).csv', keep_default_na=False)\n",
    "    print(\"CSV file alreday exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a31a17",
   "metadata": {},
   "source": [
    "# Check column characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d10322",
   "metadata": {},
   "source": [
    "## Check Rate Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b84674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Redirect',\n",
       " 'GA',\n",
       " 'B',\n",
       " 'C',\n",
       " 'List',\n",
       " 'Start',\n",
       " 'FA',\n",
       " 'Disambig',\n",
       " 'Stub',\n",
       " 'NA',\n",
       " 'FL',\n",
       " 'Portal',\n",
       " 'list',\n",
       " 'Project',\n",
       " 'A']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['rate'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07957132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>5373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Start</td>\n",
       "      <td>4440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>List</td>\n",
       "      <td>2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stub</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Disambig</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GA</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Redirect</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FA</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NA</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FL</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Project</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>list</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Portal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rate  count\n",
       "0          C   5373\n",
       "1      Start   4440\n",
       "2          B   3819\n",
       "3       List   2414\n",
       "4       Stub    737\n",
       "5   Disambig    683\n",
       "6         GA    676\n",
       "7   Redirect    584\n",
       "8         FA    381\n",
       "9         NA     82\n",
       "10        FL     15\n",
       "11   Project     12\n",
       "12      list      6\n",
       "13         A      6\n",
       "14    Portal      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rate'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ae635",
   "metadata": {},
   "source": [
    "## Check Importance Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "676a8805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NA', 'High', 'Low', 'Top', 'Mid', 'Unknown', 'Bottom']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['importance'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc58cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low</td>\n",
       "      <td>6255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mid</td>\n",
       "      <td>4169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>2319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top</td>\n",
       "      <td>2177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NA</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bottom</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  importance  count\n",
       "0        Low   6255\n",
       "1        Mid   4169\n",
       "2       High   3610\n",
       "3    Unknown   2319\n",
       "4        Top   2177\n",
       "5         NA    693\n",
       "6     Bottom      7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['importance'].value_counts().reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
