{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "326a71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import time\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07eb37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp import do_somthing, simple_worker, worker_get_bert_embedding_sentence_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89db61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fbbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b576fba",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def parallel_embedding_computation(df, num_processes=4):\n",
    "    print(\"In parallel_embedding_computation\")\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    progress_counter = manager.Value('i', 0)\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    total_rows = len(df)\n",
    "    print(f\"{total_rows} rows in df\")\n",
    "    \n",
    "    # Split data into chunks for each process\n",
    "    chunk_size = len(df) // num_processes\n",
    "    processes = []\n",
    "    \n",
    "    print(\"Start create process\")\n",
    "    for i in range(num_processes):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = len(df) if i == num_processes - 1 else (i + 1) * chunk_size\n",
    "        data_chunk = df['clean_text'][start_idx:end_idx].tolist()\n",
    "        return_dict[i] = []\n",
    "        p = mp.Process(target=worker_get_bert_embedding_sentence_based, args=(data_chunk, return_dict, i, progress_counter, total_rows, lock))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "        print(f\"Process {i} started\")\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        print(f\"Process {p.name} joined\")\n",
    "   \n",
    "    # Combine results\n",
    "    all_embeddings = []\n",
    "    for i in range(num_processes):\n",
    "        all_embeddings.extend(return_dict[i])\n",
    "\n",
    "    df['text_embedding'] = all_embeddings\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df749120",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_embedding(embedding_str):\n",
    "    # Remove the brackets and split by spaces\n",
    "    embedding_str = embedding_str.strip('[]')\n",
    "    # Add commas between the numbers\n",
    "    embedding_str = embedding_str.replace('  ', ' ')\n",
    "    embedding_str = embedding_str.replace(' ', ',')\n",
    "    # Re-add the brackets\n",
    "    embedding_str = f\"[{embedding_str}]\"\n",
    "    return embedding_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a563d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = \"[-2.63857126e-01  9.52238515e-02 -3.86462845e-02 -1.23622872e-01 4.53184135e-02  4.09205258e-02  3.81095201e-01  5.09688675e-01 9.10583660e-02 -1.22776248e-01  4.08156626e-02 -1.97499439e-01]\"\n",
    "# df['text_embedding'].iloc[0] = np.array(ast.literal_eval(preprocess_embedding(a)))\n",
    "\n",
    "# print(df.dtypes['text_embedding'])\n",
    "# print(df['text_embedding'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ccba7b3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convert_to_numpy_array(embedding_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(embedding_str), dtype=np.float32)\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing embedding: {embedding_str}, Error: {e}\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaf1a18",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(device)\n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d45fb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model with packed sequences\n",
    "class ArticleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(ArticleClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence\n",
    "        lengths = lengths.cpu()\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input)\n",
    "        # Use the hidden state from the last time step\n",
    "        output = self.fc(ht[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe798d8e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def string_to_numpy_array(input_str):\n",
    "    # Replace 'array(' with '[' and '), dtype=float32)' with ']'\n",
    "    input_str = re.sub(r'\\s+', ' ', input_str)\n",
    "    input_str = input_str.replace('array(', '').replace(', dtype=float32)', '')\n",
    "    \n",
    "    if \"array(\" in input_str:\n",
    "        start_index = input_str.find(\"array(\")\n",
    "\n",
    "        # Extract the desired substring\n",
    "        substring = input_str[start_index - 5 : start_index + len(\"array(\") + 5]\n",
    "        print(substring)\n",
    "    elif \"dtype=float32\" in input_str:\n",
    "        start_index = input_str.find(\"dtype=float32\")\n",
    "        substring = input_str[start_index - 5 : start_index + len(\"dtype=float32\") + 5]\n",
    "        print(\"Here: \", substring)\n",
    "            \n",
    "    \n",
    "    # Use ast.literal_eval to safely evaluate the string to a Python list\n",
    "    list_of_arrays = ast.literal_eval(input_str)\n",
    "    \n",
    "    # Convert the list of lists into a numpy array\n",
    "    numpy_array = np.array(list_of_arrays, dtype=np.float32)\n",
    "    \n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48179914",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# numerical_features = df[['reference', 'external_link', 'internal_link', 'table', 'formula', \n",
    "#                          'images', 'section', 'subsection', 'subsubsection', 'paragraph', 'sentence',\n",
    "#                          'flesch', 'flesch_kincaid', 'smog_index', 'coleman_liau', 'automated_readability', \n",
    "#                          'difficult_words', 'dale_chall', 'linsear', 'gunning_fog']]\n",
    "\n",
    "# classes = {'High': 1, 'Low': 0}\n",
    "# df['2_classes'] = df['2_classes'].map(classes)\n",
    "\n",
    "# # Normalize the numerical features\n",
    "# scaler = StandardScaler()\n",
    "# numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# # Combine numerical features with text embeddings\n",
    "# X = np.hstack((np.vstack(df['text_embedding'].values), numerical_features_scaled))\n",
    "# y = df['2_classes'].values\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# input_dim = X_train_tensor.shape[1]\n",
    "# hidden_dim = 64\n",
    "# output_dim = 2\n",
    "# num_layers = 2\n",
    "\n",
    "# model = LSTMClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training the model\n",
    "# num_epochs = 20\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch.unsqueeze(1))\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         outputs = model(X_batch.unsqueeze(1))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += y_batch.size(0)\n",
    "#         correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "#     print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd9dcd1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convert_embedding(dataset):\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        if not os.path.exists(f'Data/dataset_text_embedding_NO_overlap_({dataset}).pkl'):\n",
    "            df = pd.read_csv(f'Data/grouped_dataset_({dataset}).csv', keep_default_na=False)\n",
    "\n",
    "            start_time = time.time()\n",
    "            mp.set_start_method('spawn')\n",
    "            df = parallel_embedding_computation(df, num_processes=8)\n",
    "            \n",
    "            with open(f'Data/dataset_text_embedding_NO_overlap_({dataset}).pkl', 'wb') as f:\n",
    "                pickle.dump(df['text_embedding'], f)\n",
    "            \n",
    "#             df.to_csv(f'Data/dataset_text_embedding_NO_overlap_({dataset}).csv', index=False)\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Elapsed time: {elapsed_time/60:.2f} minutes\")\n",
    "        else:\n",
    "            df = pd.read_csv(f'Data/grouped_dataset_({dataset}).csv', keep_default_na=False)\n",
    "            \n",
    "            # Load the column from the .pkl file\n",
    "            with open(f'Data/dataset_text_embedding_NO_overlap_({dataset}).pkl', 'rb') as f:\n",
    "                text_embedding = pickle.load(f)\n",
    "                \n",
    "            text_embedding = pd.Series(text_embedding, index=df.index)\n",
    "            df = pd.concat([df, text_embedding.rename('text_embedding')], axis=1)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98547cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In parallel_embedding_computation\n",
      "15998 rows in df\n",
      "Start create process\n",
      "Process 0 started\n",
      "Process 1 started\n",
      "Process 2 started\n",
      "Process 3 started\n",
      "Process 4 started\n",
      "Process 5 started\n",
      "Process 6 started\n",
      "Process 7 started\n",
      "Process Process-2 joined\n",
      "Process Process-3 joined\n",
      "Process Process-4 joined\n",
      "Process Process-5 joined\n",
      "Process Process-6 joined\n",
      "Process Process-7 joined\n",
      "Process Process-8 joined\n",
      "Process Process-9 joined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBalance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mconvert_embedding\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      9\u001b[0m             df \u001b[38;5;241m=\u001b[39m parallel_embedding_computation(df, num_processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/dataset_text_embedding_NO_overlap_(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 12\u001b[0m                 \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m], f)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#             df.to_csv(f'Data/dataset_text_embedding_NO_overlap_({dataset}).csv', index=False)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m             end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "df = convert_embedding(\"Balance\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d72477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s0 = df['2_classes'][df['2_classes'] == \"High\"].sample(1051).index\n",
    "# s1 = df['2_classes'][df['2_classes'] == \"Low\"].sample(8000 - 1051).index\n",
    "# df = df.loc[s0.union(s1)]\n",
    "\n",
    "# df['text_embedding'] = df['text_embedding'].apply(string_to_numpy_array)\n",
    " \n",
    "# print(df.dtypes['text_embedding'])  # Should be 'object'\n",
    "# print(type(df['text_embedding'].iloc[0]))  # Should be <class 'numpy.ndarray'>\n",
    "# print(df['text_embedding'].iloc[0].shape)\n",
    "    \n",
    "# numerical_features = df[['reference', 'external_link', 'internal_link', 'table', 'formula', \n",
    "#                          'images', 'section', 'subsection', 'subsubsection', 'paragraph', 'sentence',\n",
    "#                          'flesch', 'flesch_kincaid', 'smog_index', 'coleman_liau', 'automated_readability', \n",
    "#                          'difficult_words', 'dale_chall', 'linsear', 'gunning_fog']]\n",
    "\n",
    "# #     classes = {'High': 1, 'Low': 0}\n",
    "# #     df['2_classes'] = df['2_classes'].map(classes)\n",
    "    \n",
    "# classes = {'FA': 0, 'GA': 1, 'B': 2, 'C': 3, 'Start': 4, 'Stub': 5}\n",
    "# df['rate'] = df['rate'].map(classes)\n",
    "    \n",
    "# # Normalize the numerical features\n",
    "# scaler = StandardScaler()\n",
    "# numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# text_embeddings = [torch.tensor(embedding) for embedding in df['text_embedding']]\n",
    "# lengths = [embedding.size(0) for embedding in text_embeddings]\n",
    "# padded_embeddings = pad_sequence(text_embeddings, batch_first=True)  # Shape: (batch_size, max_seq_length, 768)\n",
    "# print(f\"Embedding Shape: {padded_embeddings.shape}\")\n",
    "\n",
    "# num_features = numerical_features_scaled.shape[1]\n",
    "# numerical_features_tensor = torch.tensor(numerical_features_scaled, dtype=torch.float32)\n",
    "# numerical_features_expanded = numerical_features_tensor.unsqueeze(1).expand(-1, padded_embeddings.size(1), -1)  # Shape: (batch_size, max_seq_length, num_features)\n",
    "# print(f\"Numerical Features Shape: {numerical_features_expanded.shape}\")\n",
    "\n",
    "# combined_input = torch.cat((padded_embeddings, numerical_features_expanded), dim=2)  # Shape: (batch_size, max_seq_length, 768 + num_features)\n",
    "# print(f\"Concate Shape: {combined_input.shape}\")\n",
    "\n",
    "# #     labels = torch.tensor(df['2_classes'].values)\n",
    "# labels = torch.tensor(df['rate'].values)\n",
    "    \n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test, lengths_train, lengths_test = train_test_split(combined_input, labels, lengths, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert data to PyTorch tensors and create DataLoader\n",
    "# train_dataset = TensorDataset(X_train, torch.tensor(lengths_train), y_train)\n",
    "# test_dataset = TensorDataset(X_test, torch.tensor(lengths_test), y_test)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "# # Define model parameters\n",
    "# embedding_dim = 768\n",
    "# num_features = numerical_features_scaled.shape[1]\n",
    "# input_dim = embedding_dim + num_features\n",
    "# hidden_dim = 512\n",
    "# output_dim = 6\n",
    "# num_layers = 3\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "# model = ArticleClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training the model\n",
    "# num_epochs = 50\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for X_batch, lengths_batch, y_batch in train_loader:\n",
    "#         X_batch, lengths_batch, y_batch = X_batch.to(device), lengths_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch, lengths_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for X_batch, lengths_batch, y_batch in test_loader:\n",
    "#         X_batch, lengths_batch, y_batch = X_batch.to(device), lengths_batch.to(device), y_batch.to(device)\n",
    "#         outputs = model(X_batch, lengths_batch)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += y_batch.size(0)\n",
    "#         correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "#     print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc323a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9ee0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66800f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98574969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5361c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# print(df.dtypes['text_embedding'])  # Should be 'object'\n",
    "# print(type(df['text_embedding'].iloc[0]))  # Should be <class 'numpy.ndarray'>\n",
    "# print(df['text_embedding'].iloc[0].shape)\n",
    "\n",
    "# numerical_features = df[['reference', 'external_link', 'internal_link', 'table', 'formula', \n",
    "#                          'images', 'section', 'subsection', 'subsubsection', 'paragraph', 'sentence',\n",
    "#                          'flesch', 'flesch_kincaid', 'smog_index', 'coleman_liau', 'automated_readability', \n",
    "#                          'difficult_words', 'dale_chall', 'linsear', 'gunning_fog']]\n",
    "\n",
    "# # Normalize the numerical features\n",
    "# scaler = StandardScaler()\n",
    "# numerical_features_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# text_embeddings = [torch.tensor(embedding) for embedding in df['text_embedding']]\n",
    "# lengths = [embedding.size(0) for embedding in text_embeddings]\n",
    "# padded_embeddings = pad_sequence(text_embeddings, batch_first=True)  # Shape: (batch_size, max_seq_length, 768)\n",
    "# print(f\"Embedding Shape: {padded_embeddings.shape}\")\n",
    "\n",
    "# num_features = numerical_features_scaled.shape[1]\n",
    "# numerical_features_tensor = torch.tensor(numerical_features_scaled, dtype=torch.float32)\n",
    "# numerical_features_expanded = numerical_features_tensor.unsqueeze(1).expand(-1, padded_embeddings.size(1), -1)  # Shape: (batch_size, max_seq_length, num_features)\n",
    "# print(f\"Numerical Features Shape: {numerical_features_expanded.shape}\")\n",
    "\n",
    "# combined_input = torch.cat((padded_embeddings, numerical_features_expanded), dim=2)  # Shape: (batch_size, max_seq_length, 768 + num_features)\n",
    "# print(f\"Concate Shape: {combined_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [array([-1.47300020e-01,  2.19794288e-01,  3.63137573e-02, 3.15326490e-02, -8.03771734e-01,\n",
    "#         3.10615003e-01, -2.80626625e-01, -1.62438020e-01,  4.79527861e-01], dtype=float32), \n",
    "#  array([-4.09086078e-01, -1.04075804e-01,  2.62742979e-03, -2.40705684e-01, 8.03771734e-01,\n",
    "#         3.10615003e-01, -2.80626625e-01, -1.62438020e-01,  4.79527861e-01], dtype=float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b282ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import ast\n",
    "\n",
    "# # Your input string\n",
    "# input_str = \"[array([-1.47300020e-01,  2.19794288e-01,  3.63137573e-02,  7.73137435e-02, 8.94427113e-03, -3.19040328e-01,  4.53024775e-01,  6.59894586e-01],dtype=float32), array([-4.09086078e-01, -1.04075804e-01,  2.62742979e-03, -2.40705684e-01,-4.44055535e-02, -1.48875922e-01,  2.35171378e-01,  6.17567778e-01],dtype=float32)]\"\n",
    "\n",
    "# # Function to convert string to list of numpy arrays\n",
    "# def string_to_numpy_array(input_str):\n",
    "#     # Replace 'array(' with '[' and '), dtype=float32)' with ']'\n",
    "#     input_str = input_str.replace('array(', '').replace(',dtype=float32)', '')\n",
    "    \n",
    "#     # Use ast.literal_eval to safely evaluate the string to a Python list\n",
    "#     list_of_arrays = ast.literal_eval(input_str)\n",
    "    \n",
    "#     # Convert the list of lists into a numpy array\n",
    "#     numpy_array = np.array(list_of_arrays, dtype=np.float32)\n",
    "    \n",
    "#     return numpy_array\n",
    "\n",
    "# # Convert the string to numpy array\n",
    "# result_array = string_to_numpy_array(input_str)\n",
    "\n",
    "# # Print the result\n",
    "# print(result_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56113556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122a268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
